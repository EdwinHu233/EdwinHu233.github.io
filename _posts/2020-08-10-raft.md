---
layout: post
title: "论文笔记：Raft"
---

## Raft 要解决的问题

很多种分布式系统的设计都依赖于单个 master 和若干个 slave ，
例如 MapReduce , GFS 等。

为什么要采用单个 master 作为决策者呢？
主要原因是为了避免 "split brain" 现象。
例如，
假设我们设置了两个服务器 $ s_1, s_2 $ 作为决策者，
客户端的请求需要同时送给 $ s_1, s_2 $ ，
以保证二者的一致性。
但是如果由于网络原因，
客户端 $ c_1 $ 只能访问 $ s_1 $ ，
而 $ c_2 $ 则可以正常访问 $ s_1, s_2 $ 的话。
当 $ c_1 $ 向 $ s_1 $ 成功发送请求后，
$ s_1 $ 的状态被更新了，
而 $ s_2 $ 的状态则没有。
因此，$ c_2 $ 就会发现 $ s_1, s_2 $ 的不一致。
这种 "split brain" 现象的根源在于，
无法辨别“网络故障”和“服务器崩溃”这两种情况，
毕竟在发送请求的一方看来，
症状都是一样的：
发送了请求，而没有得到回应。

不过，这种以单个服务器作为决策者的设计也有缺点。
虽然单个服务器出现故障的概率比较低（相对于多个服务器而言），
但一旦发生故障，
就会导致整个系统不能正常工作。
因此，
我们希望在决策机制的设计上，
既能避免 split brain ，
又能提高容错性。

Raft 就是这样一个算法，
它将多个服务器构成的系统抽象为了单一的决策者，
以保证一致性和容错性。

为了在内部避免 split brain ，
Raft 的一条核心设计理念就是 "majority vote" 。
一个 Raft 系统包含奇数个服务器，
任何有效的状态改变都需要得到大部分服务器的同意。
在面临网络问题或部分服务器崩溃时，
只要存在能正常工作的 "majority" ，
整个系统就能继续运行。
即使 majority 中的服务器发生变动，
之前的 majority 也一定与现在的 majority 有非空的交集。
而这个交集中的服务器就可以提供信息，
以继续维持新的 majority 的一致性。

## Replicated State Machine

Replicated state machine 指的是，
在多个服务器上分别运行一套相同的状态机，
对于相同的输入，应该产生相同的输出。
通过这种冗余设计来确保分布式系统的容错性。

一般来说，replicated state machine 是通过 replicated log 实现的。
通过 consensus algorithm 保证各个服务器上 log 的一致性，
然后 state machine 从 log 中按顺序取出指令，依次执行。

在[这篇论文](/asset/raft/raft-extended.pdf)中提出的 Raft，就是一个用于保证 log 一致性的 consensus algorithm。

## The Raft Consensus Algorithm

### Basics

Raft 算法可以划分为几个相对对立的部分：
leader election, log replication, safety 和 membership changes，
在接下来几个小节依次讲解。

一个 Raft 集群包含多台服务器（例如，5 台），每个服务器的身份可能是 leader/candidate/follower 中的一种。
正常情况下，集群中恰有一个 leader ，其余都是 follower 。
而在 leader election 时，会发生 follower -> candidate 和 candidate -> leader/follower 的转变。

Raft 算法中，时间被划分为若干个不定长的 term （任期？），每个 term 由一个单调递增的整数标识。
在一个 term 的最开始，会进行 leader election 。
如果成功选举出一个 leader ，则这个 term 正常持续；
否则这个 term 会结束，开始新的 term 。
如下图所示：

![leader election](/asset/raft/leader-election.png)

每个服务器会保存一个数字 `currentTerm` ，表示自认为所在的 term 。
这个数字会在通讯时彼此交换，如果一个服务器发现自己的 `currentTerm` 比通讯对方的小，就说明自己的 `currentTerm` 过时。

- 任何服务器发现自己的 `currentTerm` 过时，都要更新自己的 `currentTerm`
- candidate/leader 如果发现自己的 `currentTerm` 过时，就会回退到 follower 状态。
- 任何服务器发现对方的 `currentTerm` 过时，都会拒绝对方的 request

Raft 的核心机制由两个 RPC 实现：

- `RequestVote`: 在选举时由 candidate 发起
- `AppendEntries`: 由 leader 发起，用于 log replication ，并作为 heartbeat

服务器会尽量并行地发送 RPC ，如果有的 RPC 没有 response ，则会重新发送。

### Leader Election

服务器启动时，身份为 follower 。
如果 follower 不断收到来自 leader/candidate 的 RPC ，那么就会始终保持在 follower 状态。
同时，leader 也会向 follower 们发送“空的”（不附加任何 log entry ） 的 `AppendEntries` 作为 heartbeat ，确保自身地位。

如果一个 follower 在一定时间（称为 election timeout ）之内没有收到 RPC 的话，就认为 leader 出现问题。
此时，follower 会将自己的 `currentTerm` 加1，并转变为 candidate 状态。
之后，这个服务器会给自己投一票，并向其他服务器发起 `RequestVote` 。
一个服务器在一个 term 内，只能给最多一个服务器投票（原则上是给第一个向自己请求的服务器投票，但 [Safety](#Safety) 一节中会引入限制条件）

Leader election 的过程中，candidate 可能会收到自称 leader 的服务器发来的 `AppendEntries` 。
这时 candidate 会根据 `currentTerm` 采取不同行动：
- 如果自己的 `currentTerm` > 对方的 `currentTerm` ，则认为对方是过气 leader ，从而拒绝对方的 RPC 请求
- 否则，认为对方是当前合法的 leader ，从而回退到 follower 状态

在一个 candidate 进行 leader election 时，会发生以下三种情况：
- 自己当选
- 其他 candidate 当选
- 没有任何 candidate 当选（有多个 candidate 获得了相同的最高投票数）。
具体来说，在 leader election 刚开始时，candidate 会重启 election timeout ，如果直到超时也没有任何 candidate 当选，
就会结束当前 term ，并开始新 term ，重新选举。

可以发现，election timeout 是一个很关键的因素。Follower 转变为 candidate 会用到这个值，candidate 重新发起选举也会用到这个值。
为了降低第三种情况的影响，在 Raft 算法的设计中，要求每个服务器的 election timeout 都是在某个区间内随机采样的。
因此，大部分情况下只会有一个 follower 发生超时并转变为 candidate ；
即使有多个 candidate 同时存在，也会在不同时间发生第二次超时，错开重新选举的时间。

### Log Replication

Leader 当选后，会接受 client request （如果有 follower 收到 client request ，则会转交给 leader ）。
每个 client request 包含一条 state machine 上的命令。

Leader 会将这条命令以及此时的 term 包装成 log entry ，追加到自己的 log 中；
然后向 followers 发送 `AppendEntries` 。

Leader 决定一条 log entry 是否能被安全地执行在 state machine 上；
这样的 log entry 被称为是 "committed" 的。
当一条 log entry 被大多数服务器复制时，leader 就认为它是 committed ，
并将其中的命令执行到自身的 state machine 上，向 client 返回结果。

那么，在 leader 执行了 entry 中的命令后，followers 又该在什么时候执行呢？
Leader 会记录 committed log entries 的最高 index ，并把这个 index 作为参数，附加在 `AppendEntries` 中。
当 follower 收到后，会将小于等于这个 index 、且尚未执行的 entries 依次执行。

![log entries](/asset/raft/log-entries.png)

在设计上，Raft 保证了一条很重要的性质 **Log Matching Property**
- 从两个 logs 中各取一个 entry ，如果它们的 term 和 index 相同，则其中包含的命令也相同（即两个 entries 完全相同）
- 从两个 logs 中各取一个 entry ，如果它们相同，则它们之前的所有 entries 都相同

第一点的证明很容易：term 相同说明这两个 entries 必然是同一个 leader 创建的，而一个 leader 不可能在同一个 index 上多次创建不同的 entry 。
第二点则是由 `AppendEntries` 的设计保证的：`AppendEntries` 不仅要求 follower 复制 log entries ，还起到了一致性检查的作用。
具体来说，如果 leader 想对 followers 的 index=i 处插入一个 entry ，则 leader 必须在 `AppendEntries` 中附加 index=i-1 这个 entry 的 term 。
如果 follower 发现 index=i-1 处的 term 不一致，则拒绝该 `AppendEntries` 。

Log 不一致的情况一般是由于服务器崩溃造成的。Follower 的 log 可能比 leader 少了某些 entries ，也可能多了某些 entries 。
例如，如果一个服务器崩溃了很长时间，就会缺少一些 entries 。而如果一个服务器曾经在某个 term 是 leader ，在创建几个 entries 后还没来得及调用 `AppendEntries` 就崩溃了，那么它在重启并成为 follower 后，就会比当前的 leader 多出一些 entries 。如下图所示：
![log inconsistence](/asset/raft/log-inconsistence.png)

为了解决 log 不一致的问题，Raft 采取的策略是：用 leader 的 log 去覆盖 followers 的 log 。
具体来说，leader 会对每个 follower 维护一个 `nextIndex` 值，表示下一次 `AppendEntries` 插入 log entry 的位置。
Leader 刚当选时，会将 `nextIndex` 都初始化为自身 log entries 最大编号 + 1 （对于上图而言，就是 12）。
然后 leader 会执行以下伪代码：
1. `AppendEntries(nextIndex, ...)`
    - 若失败，则 `nextIndex-=1` ，重复第1步
    - 若成功，则跳到第2步
2. 不断 `AppendEntries(nextIndex, ...)` 并 `nextIndex+=1` ，直到 `nextIndex` 重新回到初始值
这样，log entries 就只会由 leader 向 followers 单向流动，降低了整个算法的设计复杂性。

这样，在 leader 当选后的一段时间内，其他服务器的 log 都会被覆盖为 leader 的 log 。
由于 log entries 只会由 leader 单向流动到其他服务器，所以减少了 Raft 算法的设计复杂度。
但是，这种设计也要求 leader 在当选时必须包含之前所有 term 的 committed entries ，
否则会导致新的不一致的情况。
为了解决这个问题，下一节中引入了关于 leader election 的约束条件。

### Safety

### Follower and Candidate Crashes

### Timing and Availability
